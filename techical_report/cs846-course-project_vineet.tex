\documentclass[conference]{IEEEtran}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{url}
\usepackage{listings}

\begin{document}
\title{Continuous Function Semi-supervised Learning\\ for Sentiment Trend Detection \\from Unstructured Big Data}


\author{
	\IEEEauthorblockN{Vineet John}
	\IEEEauthorblockA{David R. Cheriton School of Computer Science\\
	University of Waterloo\\
	Waterloo, Ontario N2L 3G1\\
	Email: vineet.john@uwaterloo.ca}
}

\maketitle

\begin{abstract}
Commercial establishments (like restaurants, service centers) have several sources of feedback, most of which need not be as structured as feedback provided by services like Yelp, or Amazon. Services like these provide a fine-grained score for product and service ratings. Some sources, however, like social media (Twitter, Facebook) mailing lists (Google Groups), forums etc. are sources for less annotated data. These could be pipelined into a system that uses our prediction model, and it could help generate real-time graphs on opinion and sentiment trends. This proposal in particular, by learning a continuous function rather than a discrete one, offers a lot more depth of insight as compared to mere document classification.
\end{abstract}


% no keywords
\providecommand{\keywords}[1]{\textbf{\textit{Keywords---}} #1}
\keywords{natural language processing, big data, word embedding, regression, time-series analytics}

\IEEEpeerreviewmaketitle

\section{Introduction}
Document labelling and attribute discovery is already a widely researched area in the domain of natural language processing. The most common use-cases of document classification are product \& service review rating predictions, and automatic grading of essays. However, most of the current industrial approaches rely on learning massive amounts of contextual data from corpora and uses these trained models to associate words arrangements with their corresponding reviews\cite{bespalov2011sentiment}.

However, models like these are difficult to train for the following reasons:
\begin{itemize}
  \item Having to hand-pick features
  \item Having to manually re-train the models on subset of the features to determine the best-fit
  \item Having to use stepwise regression to eliminate non-relevant features.
\end{itemize}

All of these processes are time-consuming and the objective of this study is to explore strategies to effectively eliminate them from the process of training a language model. Instead the proposition is to rely on unsupervised strategies like continuous bag-of-words and the skip-gram model to learn word embeddings. The features thus learnt, can then be used to train a regression model to predict document scores.


\section{Similar Work}
Most of the similar work in the area of learning word-embeddings is related to classifying documents into a given set of labels or topics. The purpose of this study is to extend the usage of the word vector representation to demonstrating its usage when the response variable (the output of a learning task) a continuous function (document-scoring) rather than discrete values (document classification). This work can be viewed as a generalization of the previous approaches, because we aim to predict fine-grained scores, rather than coarse classifications.


\section{Problem Statement}
The framework is designed and developed in Python


\section{Motivation}

\subsection{Semi-supervised Learning}

\subsection{Continuous function learning}



\section{Challenges}


\section{Background}

\subsection{Word2Vec}

\subsection{Doc2Vec}

\subsection{Statistical Learning Models}

\subsection{Scaling with Spark}





\section{System Architecture}


\section{Implementation}

\subsection{Offline learning model}

\subsection{Online real-time prediction engine}

\subsection{Tech stack}


\section{Testbed}


\section{Experimental Results}


\section{Conclusions}


\section{Future Work}


\section{Acknowledgments}
The authors would like to thank...


\bibliographystyle{abbrv}
\bibliography{cs846-course-project_vineet.bib}

\end{document}
